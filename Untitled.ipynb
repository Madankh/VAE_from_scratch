{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e219d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 : average loss 230.22353763433847\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "dataset_path=\"~/datasets\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_dim = 784 # each image is 28*28 = 784 pixels\n",
    "batch_size = 100\n",
    "hidden_dim = 400\n",
    "latent_dim = 200\n",
    "learning_rate = 1e-4\n",
    "epochs = 30\n",
    "\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# MNIST\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "kwargs = {'num_workers':1, 'pin_memory':True}\n",
    "train_dataset = MNIST(dataset_path, transform=mnist_transform,train=True, download=True)\n",
    "test_dataset = MNIST(dataset_path, transform=mnist_transform, train=False, download=False)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size = batch_size, shuffle=True,**kwargs)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size = batch_size,shuffle=True , **kwargs)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim , latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Hidden layers used to process the inputs before converting to latents\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim , hidden_dim),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        # Laten representations encoded into mean and lo variance vector\n",
    "        self.mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.log_variance = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.training = True\n",
    "    def forward(self, x):\n",
    "        hidden = self.hidden_layer(x)\n",
    "        mean = self.mean(hidden)\n",
    "        log_variance = self.log_variance(hidden)\n",
    "        return mean, log_variance\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self , latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder , self).__init__()\n",
    "        \n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(latent_dim , hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim , hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim , output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        hidden = self.hidden_layer(x)\n",
    "        x_hat = torch.sigmoid(hidden)\n",
    "        return x_hat\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, Encoder , Decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder\n",
    "        self.decoder = Decoder\n",
    "    \n",
    "    def reparameterization(self , mean , variance):\n",
    "        epsilon = torch.randn_like(variance).to(device)\n",
    "        x = mean + variance * epsilon\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean , variance = self.encoder(x)\n",
    "        z = self.reparameterization(mean , torch.exp(0.5 * variance))\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat , mean , variance\n",
    "\n",
    "encoder = Encoder(input_dim=data_dim , hidden_dim=hidden_dim , latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim, hidden_dim=hidden_dim, output_dim = data_dim)\n",
    "\n",
    "model = Model(Encoder=encoder, Decoder=decoder).to(device)\n",
    "\n",
    "def bce_loss(x,x_hat, mean , variance):\n",
    "    reconstruction_loss = nn.functional.binary_cross_entropy(x_hat , x , reduction=\"sum\")\n",
    "    kl_div = -0.5*torch.sum(1+variance - mean.pow(2) - variance.exp())\n",
    "    return reconstruction_loss + kl_div\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    overall_loss = 0\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        x = x.view(batch_size , data_dim)\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x_hat , mean , variance = model(x)\n",
    "        loss = bce_loss(x, x_hat, mean, variance)\n",
    "        overall_loss +=loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"epoch {epoch + 1} : average loss {overall_loss / (batch_idx*batch_size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b0e6c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# MNIST\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "kwargs = {'num_workers':1, 'pin_memory':True}\n",
    "train_dataset = MNIST(dataset_path, transform=mnist_transform,train=True, download=True)\n",
    "test_dataset = MNIST(dataset_path, transform=mnist_transform, train=False, download=False)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size = batch_size, shuffle=True,**kwargs)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size = batch_size,shuffle=True , **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecfa627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
